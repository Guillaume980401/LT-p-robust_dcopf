{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7061a66f-d559-4656-8fdb-b7c62f21f09b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Numerical Experiments - RTS96 case\n",
    "\n",
    "\n",
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e486c8a-f138-4d49-96ad-616b3b138d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robert Mieth, 2023\n",
    "# robert.mieth@rutgers.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4850c81a-935c-472c-9cbd-1aeac2b64b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "from itertools import product\n",
    "from scipy.sparse import csr_matrix, csr_array, lil_matrix\n",
    "import scipy.sparse as scs\n",
    "import xarray as xr\n",
    "import math\n",
    "from functools import reduce\n",
    "\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "import torch\n",
    "\n",
    "# from tqdm.notebook import tqdm, trange\n",
    "from tqdm import tqdm,trange\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.patches import Ellipse\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.patches import Patch\n",
    "import matplotlib.transforms as transforms\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7676f361-10f5-4d7e-9c65-e9665bf50902",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ps_data_worker as dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3e52ed-3f2e-49e1-b49c-5c3ebeb7243b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DTYPE = torch.double\n",
    "SEED = 42\n",
    "\n",
    "# Location of data from https://github.com/GridMod/RTS-GMLC\n",
    "# expects location of \"RTS_Data\"\n",
    "RTS_DATA_DIR = DATA_LOCATION + 'data/RTS_Data'\n",
    "\n",
    "# other data\n",
    "NREL_STATIONS_METADATA_FILE = DATA_LOCATION + 'data/nrel_wind_data/wtk_site_metadata.csv'\n",
    "NREL_STATIONS_TIMEZONE_FILE = DATA_LOCATION + 'data/nrel_wind_data/site_timezone.csv'\n",
    "NREL_NC_FILES_DIR = DATA_LOCATION + 'data/nrel_wind_data/nc_files/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378c6efe-16e3-48c0-8cb3-e5ffa85827ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_station_data(station, which, return_ds=False, set_timestamps=True):\n",
    "    station_nc_file = f\"{NREL_NC_FILES_DIR}/{which}/{station}.nc\"\n",
    "    station_ds = xr.open_dataset(station_nc_file)\n",
    "    data_size = station_ds.dims['time']\n",
    "    start_time = station_ds.attrs['start_time']\n",
    "    sample_step = station_ds.attrs['sample_period']\n",
    "    timestamps_raw = np.arange(start_time, start_time+(data_size*sample_step), sample_step)\n",
    "    timestamps_utc = pd.to_datetime(timestamps_raw, unit='s', utc=True)\n",
    "    nrel_station_tz_df = pd.read_csv(NREL_STATIONS_TIMEZONE_FILE) \n",
    "    if return_ds:\n",
    "        return station_ds\n",
    "    else:\n",
    "        if set_timestamps:\n",
    "            station_df = station_ds.to_dataframe()\n",
    "            station_tz = nrel_station_tz_df.iloc[station]['zoneName']\n",
    "            timestamps_utc = pd.to_datetime(timestamps_raw, unit='s', utc=True)\n",
    "            timestamps_local = timestamps_utc.tz_convert(station_tz)\n",
    "            station_df['time_utc'] = timestamps_utc\n",
    "            station_df['time_local'] = timestamps_local\n",
    "        return station_df\n",
    "    \n",
    "def get_single_period_bus_load_vector(load_dict, act_period):\n",
    "    area_load = load_dict.query('Year == @act_period[0] and Month == @act_period[1] and Day == @act_period[2] and Period == @act_period[3]')\n",
    "    area_load = area_load[['1', '2','3']].to_dict(orient='records')[0]\n",
    "    return np.array([bus['area_load_share']*area_load[str(bus['area'])]/basemva for bus in buses])\n",
    "\n",
    "def get_single_period_data_vector(df, act_period, entry_list, data_in_pu=False):\n",
    "    act_data = df.query('Year == @act_period[0] and Month == @act_period[1] and Day == @act_period[2] and Period == @act_period[3]')\n",
    "    act_data = act_data.to_dict(orient='records')[0]\n",
    "    if data_in_pu:\n",
    "        return np.array([act_data[entry['id']] for entry in entry_list])\n",
    "    else:\n",
    "        return np.array([act_data[entry['id']]/basemva for entry in entry_list])\n",
    "\n",
    "def load_station_data(station, which, return_ds=False, set_timestamps=True):\n",
    "    station_nc_file = f\"{NREL_NC_FILES_DIR}/{which}/{station}.nc\"\n",
    "    station_ds = xr.open_dataset(station_nc_file)\n",
    "    data_size = station_ds.dims['time']\n",
    "    start_time = station_ds.attrs['start_time']\n",
    "    sample_step = station_ds.attrs['sample_period']\n",
    "    timestamps_raw = np.arange(start_time, start_time+(data_size*sample_step), sample_step)\n",
    "    timestamps_utc = pd.to_datetime(timestamps_raw, unit='s', utc=True)\n",
    "    nrel_station_tz_df = pd.read_csv(NREL_STATIONS_TIMEZONE_FILE) \n",
    "    if return_ds:\n",
    "        return station_ds\n",
    "    else:\n",
    "        if set_timestamps:\n",
    "            station_df = station_ds.to_dataframe()\n",
    "            station_tz = nrel_station_tz_df.iloc[station]['zoneName']\n",
    "            timestamps_utc = pd.to_datetime(timestamps_raw, unit='s', utc=True)\n",
    "            timestamps_local = timestamps_utc.tz_convert(station_tz)\n",
    "            station_df['time_utc'] = timestamps_utc\n",
    "            station_df['time_local'] = timestamps_local\n",
    "        return station_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3455fbc-724b-47ad-aeab-0b76e56f9e3e",
   "metadata": {},
   "source": [
    "# Prepare PS Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173e1100-f91c-4525-84af-5a0670db1652",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fa392a-75a3-43ad-b637-a46fad375191",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load NREL wind power tool kit metadata\n",
    "nrel_station_md_df = pd.read_csv(NREL_STATIONS_METADATA_FILE)\n",
    "nrel_station_tz_df = pd.read_csv(NREL_STATIONS_TIMEZONE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fe06f4-db36-47fc-9334-22c2c3678429",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load rts data\n",
    "rtsdata = dw.RTSDataSet(RTS_DATA_DIR)\n",
    "# rtsdata = dw.RTSDataSet(RTS_DATA_DIR, basemva=1.)\n",
    "psdata = dw.create_ps_data_from_rts_data(rtsdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51eaaba-eae3-462b-9451-b3ebfb44ad60",
   "metadata": {},
   "source": [
    "### Prepare system data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594de208-5ce0-456c-a6a2-ae84f814f1d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "basemva = rtsdata.basemva\n",
    "\n",
    "buses = psdata.busdata\n",
    "gens = psdata.gendata\n",
    "branches = psdata.branchdata\n",
    "\n",
    "# attach cost info\n",
    "gencost = dw.create_pwlcost_from_rts_data(rtsdata)\n",
    "gens = [dict(gen, **{'cost': gencost[i]}) for i,gen in enumerate(gens)]\n",
    "\n",
    "# separate wind farms\n",
    "wind_gen_ids = [i for i,g in enumerate(psdata.gendata) if g['fuel']=='Wind']\n",
    "winds = [gens[i] for i in wind_gen_ids]\n",
    "gens = [gens[i] for i in range(len(gens)) if i not in wind_gen_ids]\n",
    "gens_to_remove = wind_gen_ids\n",
    "# separate other gens with time-series data\n",
    "rtpv_gen_ids = [i for i,g in enumerate(psdata.gendata) if g['type']=='RTPV']\n",
    "rtpv_gens = [gens[i] for i in rtpv_gen_ids]\n",
    "gens_to_remove += rtpv_gen_ids\n",
    "pv_gen_ids = [i for i,g in enumerate(psdata.gendata) if g['type']=='PV']\n",
    "pv_gens = [gens[i] for i in pv_gen_ids]\n",
    "gens_to_remove += pv_gen_ids\n",
    "hydro_gen_ids = [i for i,g in enumerate(psdata.gendata) if g['type'] in ['HYDRO','ROR']]\n",
    "hydro_gens = [gens[i] for i in hydro_gen_ids]\n",
    "gens_to_remove += hydro_gen_ids\n",
    "# remove synchronous condensers, csp, and storage\n",
    "gens_to_remove += [i for i,g in enumerate(psdata.gendata) if g['type'] in ['SYNC_COND','CSP','STORAGE']]\n",
    "# update gens\n",
    "gens = [gens[i] for i in range(len(gens)) if i not in gens_to_remove]\n",
    "\n",
    "Nbus = len(buses)\n",
    "Ngen = len(gens)\n",
    "Nbranch = len(branches)\n",
    "Nwind = len(winds)\n",
    "\n",
    "# basic data vectors\n",
    "pmax = np.array([gen['pmax_pu'] for gen in gens])\n",
    "pmin = np.array([0 for gen in gens]) # assume this for now because we are not solving a UC \n",
    "smax = np.array([branch['cap_pu'] for branch in branches])\n",
    "\n",
    "# some maps\n",
    "def list_to_busmap(thelist):\n",
    "    locs = [entry['bus'] for entry in thelist]\n",
    "    entry_list = [i for i in range(len(thelist))]\n",
    "    return csr_matrix(([1]*len(thelist), (locs, entry_list)), shape=(Nbus, len(thelist)))\n",
    "gen2bus = list_to_busmap(gens)\n",
    "wind2bus = list_to_busmap(winds)\n",
    "pv2bus = list_to_busmap(pv_gens)\n",
    "rtpv2bus = list_to_busmap(rtpv_gens)\n",
    "hydro2bus = list_to_busmap(hydro_gens)\n",
    "\n",
    "# compute ptdf\n",
    "slack = [i for i,bus in enumerate(buses) if bus['is_slack']]\n",
    "slack_ind = slack[0]\n",
    "b_vec = np.array([1/branch['x_pu'] for branch in branches])\n",
    "b_diag = csr_matrix(np.diag(b_vec))\n",
    "lines_list = [i for i in range(Nbranch)]*2\n",
    "fromtobus = [branch['from_bus'] for branch in branches] + [branch['to_bus'] for branch in branches]\n",
    "data = [1]*Nbranch + [-1]*Nbranch \n",
    "inc_mat = csr_matrix((data, (lines_list, fromtobus)))\n",
    "B_branch = b_diag @ inc_mat\n",
    "B_bus = inc_mat.T @ B_branch\n",
    "buses_sans_slack = [i for i in range(Nbus) if i not in slack]\n",
    "B_bus_sans_slack = B_bus[buses_sans_slack][:,buses_sans_slack]\n",
    "B_bus_inv_sans_slack = scs.linalg.inv(B_bus_sans_slack)\n",
    "# add row/col of zeros before index slack\n",
    "B_bus_pseudoinv = B_bus_inv_sans_slack\n",
    "for i in slack:\n",
    "    zero_row = i # insert row before old row of that index\n",
    "    zero_col = i # insert col before old col of that index\n",
    "    B_bus_pseudoinv._shape = (B_bus_pseudoinv._shape[0]+1, B_bus_pseudoinv._shape[1]+1)\n",
    "    B_bus_pseudoinv.indices[B_bus_pseudoinv.indices >= zero_col] += 1\n",
    "    B_bus_pseudoinv.indptr = np.insert(B_bus_pseudoinv.indptr, zero_row+1, B_bus_pseudoinv.indptr[zero_row])\n",
    "ptdf = B_branch @ B_bus_pseudoinv\n",
    "ptdf.data = np.round(ptdf.data, 6)\n",
    "ptdf.eliminate_zeros()\n",
    "\n",
    "# for now only use first slope as marginal cost data\n",
    "cE = np.array([gen['cost'].slopes[0] for gen in gens]) \n",
    "cR = cE*2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fca439b-da3d-4bc4-835c-d1e215aeee28",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bbb96d-31d0-430e-a476-2b70484198ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def box_robust_dcopf_problem(mu_init, sigma_init, demand, wind, gamma=0,\n",
    "            M=1e6, allow_slack=False, quadratic_cost=False, allow_wcurt=False):\n",
    "\n",
    "    # some additional utility\n",
    "    A_base = 10\n",
    "    slack_base = 10\n",
    "    obj_base = basemva/10\n",
    "    wcurt_base = 10\n",
    "    \n",
    "    # some settings\n",
    "    FR = 1.0 # reduction of line capacity\n",
    "    \n",
    "    # define load as a parameter\n",
    "    d = cp.Parameter(Nbus, value=demand, name=\"demand\")\n",
    "    w = cp.Parameter(Nwind, value=wind, name=\"wind\")\n",
    "    \n",
    "    # define mean and uncertainty of wind power injections as parameters\n",
    "    mu = cp.Parameter(Nwind, value=mu_init, name=\"mu\")\n",
    "    sigma = cp.Parameter(Nwind, value=sigma_init, name=\"sigma\")\n",
    "        \n",
    "    # main variables\n",
    "    p  = cp.Variable(Ngen, pos=True, name=\"p\")\n",
    "    rp = cp.Variable(Ngen, pos=True, name=\"rp\")\n",
    "    rm = cp.Variable(Ngen, pos=True, name=\"rm\")\n",
    "    A  = cp.Variable((Ngen, Nwind), pos=True, name=\"A\")\n",
    "    fRAMp = cp.Variable(Nbranch, pos=True, name=\"fRAMp\")\n",
    "    fRAMm = cp.Variable(Nbranch, pos=True, name=\"fRAMm\")\n",
    "\n",
    "    # aux. variables for robust constraints\n",
    "    z = cp.Variable((2*Nbranch, Nwind), pos=True, name=\"z\")\n",
    "    \n",
    "    # aux. variables to ensure feasibility\n",
    "    if allow_slack:\n",
    "        slack = cp.Variable(2*Ngen + 2*Nbranch, pos=True, name=\"slack\")\n",
    "    if allow_wcurt:\n",
    "        wcurt = cp.Variable(Nwind, pos=True, name=\"wcurt\")\n",
    "    \n",
    "    # basic det constraints\n",
    "    if allow_wcurt:\n",
    "        flow = ptdf @ ((gen2bus @ p) + (wind2bus @ (w - wcurt/wcurt_base)) - d)\n",
    "        balconst = cp.sum(p) + cp.sum(w) - cp.sum(wcurt/wcurt_base) == cp.sum(d)\n",
    "    else:\n",
    "        flow = ptdf @ ((gen2bus @ p) + (wind2bus @ w - d))\n",
    "        balconst = cp.sum(p) + cp.sum(w) == cp.sum(d)\n",
    "    consts = [\n",
    "        balconst,\n",
    "        p + rp <= pmax,\n",
    "        p - rm >= pmin, \n",
    "        A.T @ np.ones(Ngen) == np.ones(Nwind)*A_base,\n",
    "         flow + fRAMp == smax * FR,\n",
    "        -flow + fRAMm == smax * FR,\n",
    "    ]               \n",
    "    if allow_wcurt:\n",
    "        consts.append(wcurt <= w*wcurt_base)\n",
    "\n",
    "    # box support constraints\n",
    "    for g in range(Ngen):\n",
    "        if allow_slack:\n",
    "            consts.append((mu.T @ (-A[g,:]/A_base)) + (sigma.T @ A[g,:]/A_base) <= rp[g] + slack[g]/slack_base)\n",
    "        else:\n",
    "            consts.append((mu.T @ (-A[g,:]/A_base)) + (sigma.T @ A[g,:]/A_base) <= rp[g])\n",
    "        if allow_slack:\n",
    "            consts.append((mu.T @ (A[g,:]/A_base)) + (sigma.T @  A[g,:]/A_base) <= rm[g] + slack[g+Ngen]/slack_base)\n",
    "        else:\n",
    "            consts.append((mu.T @ (A[g,:]/A_base)) + (sigma.T @  A[g,:]/A_base) <= rm[g])\n",
    "    for l in range(Nbranch):\n",
    "        Bl = cp.reshape(ptdf[l,:] @ (wind2bus - (gen2bus @ A/A_base)), Nwind)\n",
    "        # Bl = (ptdf[l,:] @ (wind2bus - (gen2bus @ A))).T\n",
    "        if allow_slack:\n",
    "            consts.append(mu.T @ Bl + (sigma.T @ z[l,:]) <= fRAMp[l] + slack[2*Ngen+l]/slack_base)\n",
    "        else:\n",
    "            consts.append(mu.T @ Bl + (sigma.T @ z[l,:]) <= fRAMp[l])\n",
    "        consts.append(z[l,:] >= Bl)\n",
    "        consts.append(z[l,:] >= -Bl)\n",
    "        if allow_slack:\n",
    "            consts.append(mu.T @ -Bl + (sigma.T @ z[Nbranch+l,:]) <= fRAMm[l] + slack[2*Ngen+Nbranch+l]/slack_base)   \n",
    "        else:\n",
    "            consts.append(mu.T @ -Bl + (sigma.T @ z[Nbranch+l,:]) <= fRAMm[l])\n",
    "        consts.append(z[Nbranch+l,:] >= -Bl)\n",
    "        consts.append(z[Nbranch+l,:] >= Bl)\n",
    "\n",
    "    # objective\n",
    "    cost_E = (cE.T @ p) * obj_base\n",
    "    if quadratic_cost:\n",
    "        cost_E_quad = cp.sum_squares(cp.multiply(cE_quad, p*obj_base))\n",
    "    else:\n",
    "        cost_E_quad = 0                         \n",
    "    (cE.T @ p)\n",
    "    cost_R = (cR.T @ (rp + rm)) * obj_base\n",
    "    objective = cost_E + cost_E_quad + cost_R\n",
    "    \n",
    "    thevars = [p, rp, rm, A, fRAMp, fRAMm, z]\n",
    "    if allow_slack:\n",
    "        thevars.append(slack)\n",
    "        penalty_slack = cp.sum(slack) * 1e3\n",
    "        # penalty_slack = cp.sum_squares(slack)\n",
    "        objective += penalty_slack\n",
    "    if allow_wcurt:\n",
    "        thevars.append(wcurt)\n",
    "        curt_penalty = cp.sum(wcurt) * 1e3/wcurt_base\n",
    "        objective += curt_penalty\n",
    "    \n",
    "    x = cp.hstack([v.flatten() for v in thevars])\n",
    "    # regularization = reg_gam*cp.sum_squares(x)\n",
    "    regularization = gamma*cp.sum_squares(A.flatten())\n",
    "    objective += regularization\n",
    "    \n",
    "    theprob = cp.Problem(cp.Minimize(objective), consts)\n",
    "\n",
    "    theparams = [d, w, mu, sigma]\n",
    "    \n",
    "    return theprob, thevars, theparams, consts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b883d3b3-75b8-4a10-b6a6-8790706155c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9146b3c-64da-4e76-83e5-630f224feb2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get a load and wind sample and test feasibility\n",
    "\n",
    "act_period = (2020,1,1,1)\n",
    "load_vec = get_single_period_bus_load_vector(rtsdata.timeseries['load_da_regional'], act_period)\n",
    "wind_vec = get_single_period_data_vector(rtsdata.timeseries['wind_da'], act_period, winds)\n",
    "pv_vec = get_single_period_data_vector(rtsdata.timeseries['pv_da'], act_period, pv_gens)\n",
    "rtpv_vec = get_single_period_data_vector(rtsdata.timeseries['rtpv_da'], act_period, rtpv_gens)\n",
    "hydro_vec = get_single_period_data_vector(rtsdata.timeseries['hydro_da'], act_period, hydro_gens)\n",
    "\n",
    "net_demand = load_vec - pv2bus@pv_vec - rtpv2bus@rtpv_vec - hydro2bus@hydro_vec\n",
    "\n",
    "print(f'Period:           {act_period}')\n",
    "print(f'Total net demand: {sum(net_demand):.2f} p.u.')\n",
    "print(f'Total wind:       {sum(wind_vec):.2f} p.u.')\n",
    "\n",
    "mu_init = np.zeros(Nwind)\n",
    "sigma_init = np.zeros(Nwind)\n",
    "\n",
    "prob, _, theparams, _ = box_robust_dcopf_problem(mu_init, sigma_init, net_demand, wind_vec, gamma=0, allow_slack=True, allow_wcurt=True)\n",
    "prob.solve(solver='ECOS')\n",
    "print(prob.status)\n",
    "print(f'Objective value:  {prob.value:.4f}')\n",
    "print()\n",
    "\n",
    "act_period = (2020,8,1,18)\n",
    "load_vec = get_single_period_bus_load_vector(rtsdata.timeseries['load_da_regional'], act_period)\n",
    "wind_vec = get_single_period_data_vector(rtsdata.timeseries['wind_da'], act_period, winds)\n",
    "pv_vec = get_single_period_data_vector(rtsdata.timeseries['pv_da'], act_period, pv_gens)\n",
    "rtpv_vec = get_single_period_data_vector(rtsdata.timeseries['rtpv_da'], act_period, rtpv_gens)\n",
    "hydro_vec = get_single_period_data_vector(rtsdata.timeseries['hydro_da'], act_period, hydro_gens)\n",
    "net_demand = load_vec - pv2bus@pv_vec - rtpv2bus@rtpv_vec - hydro2bus@hydro_vec\n",
    "\n",
    "print(f'Period:           {act_period}')\n",
    "print(f'Total net demand: {sum(net_demand):.2f} p.u.')\n",
    "print(f'Total wind:       {sum(wind_vec):.2f} p.u.')\n",
    "\n",
    "theparams[0].value = net_demand\n",
    "theparams[1].value = wind_vec\n",
    "prob.solve(solver='ECOS', warm_start=True)\n",
    "print(prob.status)\n",
    "print(f'Objective value:  {prob.value:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e81298-17c0-4b04-b9de-7727dfbbd34b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81d500f-1dea-4598-9856-a1b9db6eac18",
   "metadata": {},
   "source": [
    "### NREL wind data\n",
    "\n",
    "Original data source: https://data.openei.org/s3_viewer?bucket=nrel-pds-wtk&prefix=wtk-techno-economic%2Fpywtk-data%2F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf4b82-b25a-4ff2-8a52-099d7b635788",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load NREL wind power tool kit metadata\n",
    "nrel_station_md_df = pd.read_csv(NREL_STATIONS_METADATA_FILE)\n",
    "nrel_station_tz_df = pd.read_csv(NREL_STATIONS_TIMEZONE_FILE)\n",
    "\n",
    "# get the k=1 closest stations from the NREL wind data set\n",
    "wind_gen_latlon = [(psdata.busdata[gen['bus']]['lon'], psdata.busdata[gen['bus']]['lat']) for gen in winds]\n",
    "k = 1\n",
    "closest_stations = []\n",
    "distances = []\n",
    "for w_loc in tqdm(wind_gen_latlon):\n",
    "    dists = np.array(nrel_station_md_df.apply(lambda station: np.linalg.norm(np.array((station['longitude'], station['latitude'])) - w_loc), axis=1))\n",
    "    closest_stations.append(np.argsort(dists)[:k])\n",
    "    distances.append(np.sort(dists)[:k])\n",
    "\n",
    "if k==1:\n",
    "    closest_stations = [c[0] for c in closest_stations]\n",
    "    print(closest_stations)\n",
    "else:\n",
    "     pass\n",
    "    # not yet implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f104b2-bba2-44c7-b95d-354572b63840",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a new day ahead and real time wind for the wind generators from the NREL data set\n",
    "# 2012 seems to be the \"most similar\" in the NREL data set compared to the RTS data set\n",
    "# 5% wind scaling for better fit in the winter months\n",
    "WIND_SCALING = 1.05\n",
    "WINDYEAR = 2012\n",
    "\n",
    "wind_i = 0\n",
    "wind_gen = winds[0]\n",
    "nrel_station = closest_stations[wind_i]\n",
    "\n",
    "# get and prepare da data\n",
    "def prepare_da_wind_data_timesteps(ts):\n",
    "    return ts.year, ts.month, ts.day, ts.hour+1\n",
    "\n",
    "nrel_wind_data_da = pd.DataFrame()\n",
    "metadata = pd.DataFrame()\n",
    "for wind_i, wind_gen in enumerate(tqdm(winds)):\n",
    "    nrel_station = closest_stations[wind_i]\n",
    "    station_df_fc = load_station_data(nrel_station, 'forecasts')\n",
    "    station_cap = nrel_station_md_df.iloc[nrel_station]['capacity']\n",
    "    station_df_fc['day_ahead_power_norm'] = station_df_fc['day_ahead_power'].apply(lambda row: row/station_cap)\n",
    "    \n",
    "    if wind_i == 0:\n",
    "        nrel_wind_data_da[['Year','Month','Day','Period']] = station_df_fc.apply(lambda x: prepare_da_wind_data_timesteps(x['time_utc']), axis=1, result_type='expand')\n",
    "        metadata[['Year','Month','Day','Period']] = station_df_fc.apply(lambda x: prepare_da_wind_data_timesteps(x['time_utc']), axis=1, result_type='expand')\n",
    "    nrel_wind_data_da[wind_gen['id']] = station_df_fc['day_ahead_power_norm'] * wind_gen['pmax_pu'] * WIND_SCALING\n",
    "\n",
    "nrel_wind_data_da = nrel_wind_data_da[nrel_wind_data_da['Year'] == WINDYEAR].reset_index(drop=True)\n",
    "metadata = metadata[metadata['Year'] == WINDYEAR].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e27fb5e-e136-4c7e-a551-7e917d4f5484",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inspect data frame\n",
    "nrel_wind_data_da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e123cc2c-a8ef-4e93-b31e-a8bbbae3bd25",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Forecast errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df5ab33-3406-4ba2-ab5e-bbc8ddff788c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "station_numbers = closest_stations # legacy TODO\n",
    "station_error_collection = []\n",
    "\n",
    "wind_error_collection_df = pd.DataFrame()\n",
    "rts_wind_ids = [psdata.gendata[wind_gen_ids[si]] for si in range(len(station_numbers))]\n",
    "\n",
    "for si,station in enumerate(tqdm(station_numbers)):\n",
    "    rts_wind_gen = psdata.gendata[wind_gen_ids[si]]\n",
    "    rts_wind_id = rts_wind_gen['id']\n",
    "    \n",
    "    # load data\n",
    "    station_df_fc = load_station_data(station, 'forecasts')\n",
    "    station_df_rt = load_station_data(station, 'real_time')\n",
    "    if si == 0:\n",
    "        wind_error_collection_df['time_utc'] =  station_df_rt['time_utc']\n",
    "        wind_error_collection_df['total_da_fc'] = 0\n",
    "        \n",
    "    # prepare data\n",
    "    station_cap = nrel_station_md_df.iloc[station]['capacity']\n",
    "    station_df_fc['day_ahead_power_norm'] = station_df_fc['day_ahead_power'].apply(lambda row: row/station_cap)\n",
    "    station_df_fc['hour_ahead_power_norm'] = station_df_fc['hour_ahead_power'].apply(lambda row: row/station_cap)\n",
    "    station_df_rt['power_norm'] = station_df_rt['power'].apply(lambda row: row/station_cap)\n",
    "    # north wind (i.e., wind FROM the north) is 360°, south wind is 180°\n",
    "    # mathematical wind direction has west wind aligned with the x-axis, i.e., zero vector angle for west wind (270°)\n",
    "    station_df_rt['wind_direction_ang'] = station_df_rt['wind_direction'].apply(lambda wwd: math.sin(270-wwd)) \n",
    "    # compute forecast errors for day_ahead\n",
    "    # expand the forecast to 5 mins\n",
    "    day_ahead_5min = np.repeat(station_df_fc['day_ahead_power_norm'].to_numpy(), 12)\n",
    "    rt_5min = station_df_rt['power_norm'].to_numpy()\n",
    "    station_df_rt['error_day_ahead'] = rt_5min - day_ahead_5min\n",
    "    # compute forecast errors for hour_ahead\n",
    "    # expand the forecast to 5 mins\n",
    "    hour_ahead_5min = np.repeat(station_df_fc['hour_ahead_power_norm'].to_numpy(), 12)\n",
    "    rt_5min = station_df_rt['power_norm'].to_numpy()\n",
    "    station_df_rt['error_hour_ahead'] = rt_5min - hour_ahead_5min\n",
    "\n",
    "    # re-construct forecast value\n",
    "    station_df_rt['da_forecast'] = station_df_rt['power_norm'] - station_df_rt['error_day_ahead']\n",
    "    \n",
    "    # aggregate\n",
    "    wind_error_collection_df['total_da_fc'] += station_df_rt['da_forecast'] * rts_wind_gen['pmax_pu']\n",
    "    wind_error_collection_df[rts_wind_id] = station_df_rt['error_day_ahead'] * rts_wind_gen['pmax_pu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5fb9d8-a346-4189-ba16-08f03168dd61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DTYPE = torch.float32\n",
    "TEST_PERC = 0.25\n",
    "\n",
    "print(\">> preparing time stamps\")\n",
    "wind_error_collection_df[['Year','Month','Day','Period']] = wind_error_collection_df.apply(lambda x: prepare_da_wind_data_timesteps(x['time_utc']), axis=1, result_type='expand')\n",
    "\n",
    "print(\">> preparing data\")\n",
    "all_errors = wind_error_collection_df[['309_WIND_1', '317_WIND_1', '303_WIND_1', '122_WIND_1']].to_numpy()\n",
    "all_periods = wind_error_collection_df[['Year','Month','Day','Period']].to_numpy()\n",
    "\n",
    "train_ids, test_ids = train_test_split(np.arange(len(wind_error_collection_df)), test_size=int(len(wind_error_collection_df)*TEST_PERC), random_state=SEED)\n",
    "\n",
    "# forecast error data \n",
    "train = all_errors[train_ids]\n",
    "test = all_errors[test_ids]\n",
    "\n",
    "print(f'Created {len(train)} training samples an {len(test)} test samples')\n",
    "\n",
    "train_data = torch.tensor(train, dtype=DTYPE)\n",
    "test_data = torch.tensor(test, dtype=DTYPE)\n",
    "\n",
    "# periods for selection later\n",
    "train_scenarios = all_periods[train_ids]\n",
    "test_scenarios = all_periods[test_ids]\n",
    "\n",
    "# init based on stdv\n",
    "mu_init = np.mean(train, axis=0)\n",
    "sigma_init =np.std(train, axis=0)/2\n",
    "\n",
    "# init based on percentile\n",
    "perc= 10 # in percent\n",
    "percupper = np.percentile(train, 100-perc, axis=0)\n",
    "perclower = np.percentile(train, perc, axis=0)\n",
    "mu_base_perc = (percupper + perclower) / 2\n",
    "sigma_base_perc = mu_init + ((percupper - perclower) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad500b00-af6b-4269-895c-34b9bca15efd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cost-based loss\n",
    "\n",
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad03d1c-6a48-4a22-91f6-a4938d7f5d1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use relu to discard negative values\n",
    "nonneg = torch.nn.ReLU(inplace=False)\n",
    "\n",
    "def expected_cost(var_values, hist_data_tch, gamma=0):\n",
    "    \n",
    "    # some settings\n",
    "    A_base = 10\n",
    "    slack_base = 10\n",
    "    obj_base = basemva/10\n",
    "    wcurt_base = 10\n",
    "    \n",
    "    p = var_values[varname_to_varid_dict['p']]\n",
    "    rp = var_values[varname_to_varid_dict['rp']]\n",
    "    rm = var_values[varname_to_varid_dict['rm']]\n",
    "    A = var_values[varname_to_varid_dict['A']]\n",
    "    fRAMp = var_values[varname_to_varid_dict['fRAMp']]\n",
    "    fRAMm = var_values[varname_to_varid_dict['fRAMm']]\n",
    "    if 'slack' in varname_to_varid_dict.keys():\n",
    "        slack = var_values[varname_to_varid_dict['slack']]\n",
    "    else:\n",
    "        slack = torch.tensor(0)\n",
    "    if 'wcurt' in varname_to_varid_dict.keys():\n",
    "        wcurt = var_values[varname_to_varid_dict['wcurt']]\n",
    "    else:\n",
    "        wcurt = torch.tensor(0)\n",
    "\n",
    "    varlist = [p, rp, rm, A, fRAMp, fRAMm, wcurt, slack]\n",
    "    x = torch.hstack([v.flatten() for v in varlist])\n",
    "        \n",
    "    # expected first stage cost\n",
    "    opf_cost = (torch.dot(p, cE_tch) + torch.dot(rp + rm, cR_tch)) \n",
    "    opf_cost += torch.sum(wcurt) * 1e3/wcurt_base \n",
    "    opf_cost += torch.sum(slack) * obj_base * 1e3\n",
    "\n",
    "    # expected reserve violation cost\n",
    "    reaction_gen = torch.matmul(A/A_base, hist_data_tch.T)\n",
    "    expected_rp_viol_cost = torch.sum(nonneg(-reaction_gen.T - rp[None, :]).mean(axis=0) * cM_tch)\n",
    "    expected_rm_viol_cost = torch.sum(nonneg(reaction_gen.T - rm[None, :]).mean(axis=0) * cM_tch)\n",
    "    reaction_branch = torch.matmul(torch.matmul(ptdf_tch,(wind2bus_tch - torch.matmul(gen2bus_tch, A/A_base))), hist_data_tch.T)\n",
    "    expected_framp_viol_cost = torch.sum(nonneg(reaction_branch.T - fRAMp[None, :]).mean(axis=0) * cM_tch)\n",
    "    expected_framm_viol_cost = torch.sum(nonneg(-reaction_branch.T - fRAMm[None, :]).mean(axis=0) * cM_tch)\n",
    "    \n",
    "    regularization = gamma*torch.sum(torch.square(x))\n",
    "    \n",
    "    return opf_cost + expected_rp_viol_cost + expected_rm_viol_cost + expected_framp_viol_cost + expected_framm_viol_cost + regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e340016-75ab-4b2a-bd79-57d12531e3d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### P-All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a5c920-e85c-4075-a972-ee4b352e358a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_label = 'with_prescription_lr-5_gamm0_cM2000'\n",
    "\n",
    "# some settings\n",
    "MAX_EPOCH = 100\n",
    "BATCHSIZE = 10\n",
    "LR = 1e-6\n",
    "LMOM = 0.3\n",
    "GAMMA = 0.\n",
    "\n",
    "# reset randomness\n",
    "np.random.seed(seed=SEED)\n",
    "\n",
    "# constraint violation penalty\n",
    "cM = 10000\n",
    "\n",
    "# other parameters\n",
    "cE_tch, cR_tch, cM_tch = torch.tensor(cE, dtype=DTYPE), torch.tensor(cR, dtype=DTYPE), torch.tensor(cM, dtype=DTYPE)\n",
    "ptdf_tch = torch.tensor(ptdf.toarray(), dtype=DTYPE)\n",
    "gen2bus_tch = torch.tensor(gen2bus.toarray(), dtype=DTYPE)\n",
    "wind2bus_tch = torch.tensor(wind2bus.toarray(), dtype=DTYPE)\n",
    "zero_tch = torch.tensor(0, dtype=DTYPE)\n",
    "\n",
    "# get an intial scenario\n",
    "act_period = (2020,1,1,1)\n",
    "load_vec = get_single_period_bus_load_vector(rtsdata.timeseries['load_da_regional'], act_period)\n",
    "wind_vec = get_single_period_data_vector(rtsdata.timeseries['wind_da'], act_period, winds)\n",
    "pv_vec = get_single_period_data_vector(rtsdata.timeseries['pv_da'], act_period, pv_gens)\n",
    "rtpv_vec = get_single_period_data_vector(rtsdata.timeseries['rtpv_da'], act_period, rtpv_gens)\n",
    "hydro_vec = get_single_period_data_vector(rtsdata.timeseries['hydro_da'], act_period, hydro_gens)\n",
    "net_demand = load_vec - pv2bus@pv_vec - rtpv2bus@rtpv_vec - hydro2bus@hydro_vec\n",
    "\n",
    "# set up the layer\n",
    "inner, vs, params, consts = box_robust_dcopf_problem(mu_init, sigma_init, net_demand, wind_vec, gamma=GAMMA, allow_slack=True, allow_wcurt=True, quadratic_cost=False)\n",
    "inner_cvxpylayer = CvxpyLayer(inner, parameters=inner.parameters(), variables=inner.variables())\n",
    "varname_to_varid_dict = {key: i for i,key in enumerate(inner.var_dict.keys())}\n",
    "\n",
    "# set up the prescriptor\n",
    "sigma_prescriptor = torch.nn.Linear(Nbus+Nwind, Nwind)\n",
    "sigma_prescriptor.weight.data = torch.zeros((Nwind, Nbus+Nwind))\n",
    "sigma_prescriptor.bias.data = torch.tensor(sigma_init, dtype=DTYPE)\n",
    "mu_prescriptor = torch.nn.Linear(Nbus+Nwind, Nwind)\n",
    "mu_prescriptor.weight.data = torch.zeros((Nwind, Nbus+Nwind))\n",
    "mu_prescriptor.bias.data = torch.tensor(mu_init, dtype=DTYPE)\n",
    "\n",
    "# set up SGD \n",
    "parameters = list(mu_prescriptor.parameters()) + list(sigma_prescriptor.parameters())\n",
    "opt = torch.optim.SGD(parameters, lr=LR, momentum=LMOM) \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, patience=5)\n",
    "\n",
    "#train\n",
    "loss_during_training = []\n",
    "training_data_df = pd.DataFrame()\n",
    "# for epoch in trange(MAX_EPOCH, desc=\"Training progress\", unit=\"epoch\"):\n",
    "with trange(MAX_EPOCH, position=0) as ep_looper:\n",
    "    for epoch in ep_looper:\n",
    "        ep_looper.set_description(f'Epoch {epoch}')\n",
    "        \n",
    "        # reset loss\n",
    "        loss = torch.tensor(0., dtype=DTYPE)\n",
    "        oosloss = torch.tensor(0., dtype=DTYPE)\n",
    "        baseloss = torch.tensor(0., dtype=DTYPE)\n",
    "    \n",
    "        with trange(BATCHSIZE, position=1, leave=False) as batch_looper:\n",
    "            for batch in batch_looper:\n",
    "                # create net demand scenario \n",
    "                rts_year = 2020\n",
    "                wind_year = 2012\n",
    "                scen_mo = np.random.randint(1,12)\n",
    "                scen_day = np.random.randint(1,28)\n",
    "                scen_per = np.random.randint(1,24)\n",
    "                act_period = (rts_year, scen_mo, scen_day, scen_per)\n",
    "                act_wind = (wind_year, scen_mo, scen_day, scen_per)\n",
    "                load_vec = get_single_period_bus_load_vector(rtsdata.timeseries['load_da_regional'], act_period)\n",
    "                wind_vec = get_single_period_data_vector(nrel_wind_data_da, act_wind, winds)\n",
    "                pv_vec = get_single_period_data_vector(rtsdata.timeseries['pv_da'], act_period, pv_gens)\n",
    "                rtpv_vec = get_single_period_data_vector(rtsdata.timeseries['rtpv_da'], act_period, rtpv_gens)\n",
    "                hydro_vec = get_single_period_data_vector(rtsdata.timeseries['hydro_da'], act_period, hydro_gens)\n",
    "                net_demand = load_vec - pv2bus@pv_vec - rtpv2bus@rtpv_vec - hydro2bus@hydro_vec\n",
    "                d_scenario = torch.tensor(net_demand, dtype=DTYPE)\n",
    "                w_scenario = torch.tensor(wind_vec, dtype=DTYPE)\n",
    "                scenario_vector = torch.cat((d_scenario, w_scenario))\n",
    "\n",
    "                d_scenario = torch.tensor(net_demand, dtype=DTYPE)\n",
    "                w_scenario = torch.tensor(wind_vec, dtype=DTYPE)\n",
    "                scenario_vector = torch.cat((d_scenario, w_scenario))\n",
    "\n",
    "                # prescribe the the set size\n",
    "                mu = mu_prescriptor(scenario_vector.float())\n",
    "                sigma = sigma_prescriptor(scenario_vector.float())\n",
    "\n",
    "                # compute current inner solution\n",
    "                opf_params = [w_scenario, d_scenario, mu, sigma]\n",
    "                var_values = inner_cvxpylayer(*opf_params,  solver_args={'solve_method': \"ECOS\"})\n",
    "\n",
    "                # calculate loss\n",
    "                temploss = expected_cost(var_values, train_data, gamma=GAMMA)\n",
    "                loss = loss + temploss/BATCHSIZE\n",
    "              \n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "\n",
    "        # step the SGD\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        scheduler.step(loss)\n",
    "        \n",
    "        # some analysis and reporting\n",
    "        current_results = pd.Series({\n",
    "            \"epoch\": epoch,\n",
    "            \"loss\": loss.item(),\n",
    "            \"mu_pres_weight\": mu_prescriptor.weight.data.detach(),\n",
    "            \"mu_pres_bias\": mu_prescriptor.bias.data.detach(),\n",
    "            \"sigma_pres_weight\": sigma_prescriptor.weight.data.detach(),\n",
    "            \"sigma_pres_bias\": sigma_prescriptor.bias.data.detach(),\n",
    "        })\n",
    "        training_data_df = pd.concat([training_data_df, current_results.to_frame().T], ignore_index=True)\n",
    "        training_data_df.to_csv(f\"{experiment_label}.csv\")\n",
    "        loss_during_training.append(loss.item())\n",
    "        ep_looper.set_postfix(loss=loss.item())\n",
    "\n",
    "results_with_prescription = training_data_df.copy()\n",
    "            \n",
    "# some final reporting    \n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(loss_during_training, label='train')\n",
    "ax.set_ylabel('loss')\n",
    "ax.set_xlabel('step')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ba666b-58d2-4265-835c-cda7d4803fb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### OOS Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40375307-200d-4137-bbc8-af8e84ef5955",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.\n",
    "\n",
    "# reset randomness\n",
    "np.random.seed(seed=2)\n",
    "\n",
    "cM = 10000\n",
    "cM_tch = torch.tensor(cM, dtype=DTYPE)\n",
    "\n",
    "# get final training\n",
    "def get_prescriptors(training_results):\n",
    "    final_training_epoch = training_results.iloc[-1]\n",
    "    sigma_prescriptor = torch.nn.Linear(Nbus+Nwind, Nwind)\n",
    "    sigma_prescriptor.weight.data = final_training_epoch.sigma_pres_weight\n",
    "    sigma_prescriptor.bias.data = final_training_epoch.sigma_pres_bias\n",
    "    mu_prescriptor = torch.nn.Linear(Nbus+Nwind, Nwind)\n",
    "    mu_prescriptor.weight.data = final_training_epoch.mu_pres_weight\n",
    "    mu_prescriptor.bias.data = final_training_epoch.mu_pres_bias\n",
    "    return mu_prescriptor, sigma_prescriptor\n",
    "\n",
    "# other parameters\n",
    "cE_tch, cR_tch, cM_tch = torch.tensor(cE, dtype=DTYPE), torch.tensor(cR, dtype=DTYPE), torch.tensor(cM, dtype=DTYPE)\n",
    "ptdf_tch = torch.tensor(ptdf.toarray(), dtype=DTYPE)\n",
    "gen2bus_tch = torch.tensor(gen2bus.toarray(), dtype=DTYPE)\n",
    "wind2bus_tch = torch.tensor(wind2bus.toarray(), dtype=DTYPE)\n",
    "zero_tch = torch.tensor(0, dtype=DTYPE)\n",
    "\n",
    "# create model\n",
    "act_period = (2020,1,1,1)\n",
    "load_vec = get_single_period_bus_load_vector(rtsdata.timeseries['load_da_regional'], act_period)\n",
    "wind_vec = get_single_period_data_vector(rtsdata.timeseries['wind_da'], act_period, winds)\n",
    "pv_vec = get_single_period_data_vector(rtsdata.timeseries['pv_da'], act_period, pv_gens)\n",
    "rtpv_vec = get_single_period_data_vector(rtsdata.timeseries['rtpv_da'], act_period, rtpv_gens)\n",
    "hydro_vec = get_single_period_data_vector(rtsdata.timeseries['hydro_da'], act_period, hydro_gens)\n",
    "net_demand = load_vec - pv2bus@pv_vec - rtpv2bus@rtpv_vec - hydro2bus@hydro_vec\n",
    "prob, thevars, theparams, _ = box_robust_dcopf_problem(mu_init, sigma_init, net_demand, wind_vec, gamma=GAMMA, allow_slack=True, quadratic_cost=False, allow_wcurt=True)\n",
    "varname_to_varid_dict = {key: i for i,key in enumerate(prob.var_dict.keys())}\n",
    "\n",
    "pu_scale = 100\n",
    "N_OOS = 100\n",
    "oos_loss_base = []\n",
    "oos_loss_presc = []\n",
    "oos_sets_presc = []\n",
    "oos_errors = []\n",
    "for oos in trange(N_OOS):\n",
    "\n",
    "    # create a scenario\n",
    "    rts_year = 2020\n",
    "    wind_year = 2012\n",
    "    scen_mo = np.random.randint(1,12)\n",
    "    scen_day = np.random.randint(1,28)\n",
    "    scen_per = np.random.randint(1,24)\n",
    "    act_period = (rts_year, scen_mo, scen_day, scen_per)\n",
    "    act_wind = (wind_year, scen_mo, scen_day, scen_per)\n",
    "    load_vec = get_single_period_bus_load_vector(rtsdata.timeseries['load_da_regional'], act_period)\n",
    "    wind_vec = get_single_period_data_vector(nrel_wind_data_da, act_wind, winds)\n",
    "    pv_vec = get_single_period_data_vector(rtsdata.timeseries['pv_da'], act_period, pv_gens)\n",
    "    rtpv_vec = get_single_period_data_vector(rtsdata.timeseries['rtpv_da'], act_period, rtpv_gens)\n",
    "    hydro_vec = get_single_period_data_vector(rtsdata.timeseries['hydro_da'], act_period, hydro_gens)\n",
    "    net_demand = load_vec - pv2bus@pv_vec - rtpv2bus@rtpv_vec - hydro2bus@hydro_vec\n",
    "    d_scenario = torch.tensor(net_demand, dtype=DTYPE)\n",
    "    w_scenario = torch.tensor(wind_vec, dtype=DTYPE)\n",
    "    scenario_vector = torch.cat((d_scenario, w_scenario))\n",
    "    \n",
    "    # create a single error occurence\n",
    "    scenario_id = np.random.randint(0,len(test_data))\n",
    "    cur_data = test_data[scenario_id]\n",
    "\n",
    "    ## set base parameters\n",
    "    theparams[0].value = net_demand\n",
    "    theparams[1].value = wind_vec\n",
    "    \n",
    "    ## base model\n",
    "    theparams[2].value = mu_base_perc\n",
    "    theparams[3].value = sigma_base_perc\n",
    "    prob.solve(solver='ECOS', warm_start=True)\n",
    "    var_values = [torch.tensor(v.value, dtype=DTYPE) for v in prob.variables()]\n",
    "    loss_base = expected_cost(var_values, cur_data, gamma=GAMMA)\n",
    "    oos_loss_base.append(loss_base.item() * pu_scale)\n",
    "    \n",
    "    ## prescribed model\n",
    "    # parametrize prescribed model\n",
    "    mu_presc, sigma_presc = get_prescriptors(results_with_prescription)\n",
    "    mu_current = mu_presc(scenario_vector).detach().numpy()\n",
    "    sigma_current = sigma_presc(scenario_vector).detach().numpy()\n",
    "    theparams[2].value = mu_current\n",
    "    theparams[3].value = sigma_current\n",
    "    prob.solve(solver='ECOS', warm_start=True)\n",
    "    var_values = [torch.tensor(v.value, dtype=DTYPE) for v in prob.variables()]\n",
    "    loss_presc = expected_cost(var_values, cur_data, gamma=GAMMA)\n",
    "    oos_loss_presc.append(loss_presc.item() * pu_scale)\n",
    "    \n",
    "    oos_sets_presc.append({\"mu\": mu_current, \"sigma\": sigma_current})\n",
    "    oos_errors.append(cur_data)\n",
    " \n",
    "print(f'Expected cost base: {np.mean(oos_loss_base):.3f}')\n",
    "print(f'Expected cost P-All: {np.mean(oos_loss_presc):.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-context_learning",
   "language": "python",
   "name": "venv-context_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
